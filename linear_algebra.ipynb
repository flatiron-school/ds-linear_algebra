{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#What-&amp;-Why-of-Linear-Algebra\" data-toc-modified-id=\"What-&amp;-Why-of-Linear-Algebra-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What &amp; Why of Linear Algebra</a></span></li><li><span><a href=\"#Scalars,-Vectors,-and-Tensors!-Oh-My!\" data-toc-modified-id=\"Scalars,-Vectors,-and-Tensors!-Oh-My!-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Scalars, Vectors, and Tensors! Oh My!</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scalars\" data-toc-modified-id=\"Scalars-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Scalars</a></span></li><li><span><a href=\"#Vectors\" data-toc-modified-id=\"Vectors-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Vectors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Code-for-Vectors\" data-toc-modified-id=\"Code-for-Vectors-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Code for Vectors</a></span></li><li><span><a href=\"#Math-with-Vectors\" data-toc-modified-id=\"Math-with-Vectors-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Math with Vectors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vector-Addition\" data-toc-modified-id=\"Vector-Addition-3.2.2.1\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;</span>Vector Addition</a></span></li><li><span><a href=\"#Vector-Multiplication\" data-toc-modified-id=\"Vector-Multiplication-3.2.2.2\"><span class=\"toc-item-num\">3.2.2.2&nbsp;&nbsp;</span>Vector Multiplication</a></span></li></ul></li></ul></li><li><span><a href=\"#Matrices-and-Tensors\" data-toc-modified-id=\"Matrices-and-Tensors-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Matrices and Tensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Code-for-Matrices-and-Tensors\" data-toc-modified-id=\"Code-for-Matrices-and-Tensors-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Code for Matrices and Tensors</a></span></li><li><span><a href=\"#Math-with-Tensors\" data-toc-modified-id=\"Math-with-Tensors-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Math with Tensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Addition\" data-toc-modified-id=\"Addition-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Addition</a></span></li><li><span><a href=\"#Dot-Product\" data-toc-modified-id=\"Dot-Product-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Dot-Product</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#More-with-Matrices\" data-toc-modified-id=\"More-with-Matrices-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>More with Matrices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identity-Matrices\" data-toc-modified-id=\"Identity-Matrices-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Identity Matrices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identity-Matrices-in-NumPy\" data-toc-modified-id=\"Identity-Matrices-in-NumPy-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Identity Matrices in NumPy</a></span></li></ul></li><li><span><a href=\"#Inverse-Matrices\" data-toc-modified-id=\"Inverse-Matrices-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Inverse Matrices</a></span></li></ul></li><li><span><a href=\"#Solving-a-System-of-Linear-Equations\" data-toc-modified-id=\"Solving-a-System-of-Linear-Equations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Solving a System of Linear Equations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Representing-the-System-with-Matrices\" data-toc-modified-id=\"Representing-the-System-with-Matrices-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Representing the System with Matrices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Coding-It-with-NumPy\" data-toc-modified-id=\"Coding-It-with-NumPy-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Coding It with NumPy</a></span></li><li><span><a href=\"#Solve-It-Faster-with-NumPy's-linalg.solve()\" data-toc-modified-id=\"Solve-It-Faster-with-NumPy's-linalg.solve()-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Solve It Faster with NumPy's <code>linalg.solve()</code></a></span></li></ul></li></ul></li><li><span><a href=\"#Solving-for-the-Line-of-Best-Fit:-Linear-Regression\" data-toc-modified-id=\"Solving-for-the-Line-of-Best-Fit:-Linear-Regression-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Solving for the Line of Best Fit: Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Algebra-Solves-the-Best-Fit-Line-Problem\" data-toc-modified-id=\"Linear-Algebra-Solves-the-Best-Fit-Line-Problem-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Linear Algebra Solves the Best-Fit Line Problem</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linalg](https://www.engineer4free.com/uploads/1/0/2/9/10296972/3295580_orig.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `numpy` to construct and manipulate scalars, vectors, and matrices\n",
    "- Use `numpy` to construct and manipulate identity matrices and inverse matrices\n",
    "- Use `numpy` to solve systems of equations\n",
    "- Describe the matrix manipulations required to solve the best-fit line problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What & Why of Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are a fundamental aspect of data science models and problems, including image processing, deep learning, NLP, and PCA. You will encounter matrices *many* times in your career as a data scientist. Matrices are a fundamental tool in **linear algebra**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Study of \"vector spaces\"; relationship of **linear** relationships\n",
    "- Uses vectors, matrices, and tensors\n",
    "- Mapping & dimensionality (PCA)\n",
    "- Used in lots of ML applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to put abstract ideas into the formalism of linear algebra, such as:\n",
    "- data values\n",
    "- images/pixels\n",
    "- language (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, Vectors, and Tensors! Oh My!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/different_tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can think of these values being built up to higher dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _scalar_ has simply a single value. Any real number can be the value of a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar\n",
    "s = np.arange(1)\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _vector_ can be specified by with just _two_ parameters: magnitude and direction. To remind of this direction, a vector is typically denoted with an arrow above the variable representing it: $\\vec{v}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Cartesian coordinate system, a vector $\\vec{v}$ will often be specified by the components defined by the coordinate system. \n",
    "\n",
    "In this way a vector can be embedded in a higher-dimensional space, which allows us to speak of vectors of any dimension (even though, as directed line segments, all vectors are, strictly speaking, one-dimensional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/b/ba/Vector-length.png)\n",
    "\n",
    "> <a href=\"https://commons.wikimedia.org/wiki/File:Vector-length.png\">Svjo</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on vectors, see this helpful [video](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can think of vectors as a one-dimensional object residing in multidimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector\n",
    "v = np.arange(4)\n",
    "display(v)\n",
    "print('Shape:', np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector\n",
    "v = np.array([1, 0, 2, 1, 0, 1, 0, 1, 1])\n",
    "display(v)\n",
    "print('Shape:', np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ways to define vector\n",
    "x = np.linspace(-np.pi, np.pi, 10)\n",
    "display(x)\n",
    "print('Shape:', np.shape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors above are usually called _row vectors_.\n",
    "\n",
    "We can also specify _column vectors_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column vector from scratch\n",
    "col_vector = np.array([[1], [0], [2], [1], [0], [1], [0], [1], [1]])\n",
    "display(col_vector)\n",
    "print('Shape:', np.shape(col_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting or \"reshaping\" a row vector to a column vector\n",
    "display(v)\n",
    "print('Shape:', np.shape(v))\n",
    "\n",
    "\n",
    "print('')\n",
    "print('='*64)\n",
    "\n",
    "col_vector = v.reshape(-1, 1) # This is a common way for row -> column\n",
    "display(col_vector)\n",
    "print('Shape:', np.shape(col_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math with Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector addition is simple: Just add the corresponding components together:\n",
    "\n",
    "$[8, 14] + [7, 6] = [15, 20]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Python is not particularly good for non-scalar arithmetic. Make a general practice of turning to `numpy` for mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8, 14] + [7, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array([8, 14])\n",
    "vec_2 = np.array([7, 6])\n",
    "vec_1 + vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact there are multiple ways of understanding the notion of vector multiplication. All are potentially useful, but the one that we'll likely be of most use is the *dot-product*, which is defined as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    ". \n",
    "\\begin{bmatrix}\n",
    "c \\\\\n",
    "d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "ac + bd\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot-product is the sum of the pariwise products of the vectors' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the different attributes and methods available to\n",
    "# a NumPy array.\n",
    "\n",
    "# vec_1.\n",
    "\n",
    "# There are many options. Notice that one of these options is 'dot'.\n",
    "# This is our dot-product! So let's use the .dot() method to calculate\n",
    "# the dot-product of our two vectors:\n",
    "\n",
    "vec_1.dot(vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*7 + 14*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use '@'\n",
    "\n",
    "vec_1 @ vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For higher dimensions we can use **matrices** to express ourselves. Suppose we had a two-variable system:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{1,1}x_1 + a_{1,2}x_2 = c_1 \\\\\n",
    "a_{2,1}x_1 + a_{2,2}x_2 = c_2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrices, we can write this as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$A\\vec{x} = \\vec{c}$,\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\vec{x}$ is the _vector_ $(x_1, x_2)$;\n",
    "- $\\vec{c}$ is the _vector_ $(c_1, c_2)$; and\n",
    "- $A$ is the _matrix_ of coefficients that describe our system:\n",
    "\n",
    "$\\begin{equation} A = \n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, all of what we've talked about are **tensors**, just different ranks:\n",
    "\n",
    "- scalar == 0th rank tensor\n",
    "- vector == 1st rank tensor\n",
    "    + vectors are made up from a \"list\" of scalars\n",
    "- matrix == 2nd rank tensor\n",
    "    + matrices are made up from a \"list\" of vectors\n",
    "- 3D matrix == 3rd rank tensor\n",
    "    + 3D matrices are made up from a \"list\" of (2D) matrices\n",
    "- and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, people are referring to the higher ranked tensors (2nd and above) when they say \"tensor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Matrices and Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "M = np.arange(4 * 2).reshape((4, 2))\n",
    "display(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Tensor\n",
    "T_3d = np.arange(4 * 2 * 3).reshape((4, 2, 3))\n",
    "display(T_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D Tensor\n",
    "T_4d = np.arange(4 * 2 * 3).reshape((2, 2, 2, 3))\n",
    "display(T_4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of matrices is straightforward: Just add corresponding elements. In order to add two matrices $A$ and $B$, they must have the same number of rows and columns:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]]) \n",
    "B = np.array([[4, 3], [2, 1]])\n",
    "display(A)\n",
    "display(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot-Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just as there were different notions of \"multiplication\" for vectors, so too there are different notions of multiplication for matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often when people talk about multiplying matrices they'll mean the dot-product:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the entries in each *row* of the left matrix and multiply them, respectively, by the entries in each *column* of the right matrix, and then add them up. This is the product we calculated above with our two vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can multiply in NumPy in the same way of doing a dot product with vectors.\n",
    "\n",
    "```python\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.array([[11, 22], [33, 44], [55, 66]])\n",
    "\n",
    "# Different ways to do the same dot product\n",
    "AB = np.dot(A,B)\n",
    "AB = A.dot(B)\n",
    "AB = A @ B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe also that in order to be able to perform the dot product on two matrices A and B, **the number of columns of A must equal the number of rows of B**.\n",
    "\n",
    "Also, **the number of rows of the product matrix will equal the number of rows of A, and the number of columns of the product matrix will equal the number of columns of B**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **A note about vectors and matrices**\n",
    ">\n",
    "> Matrix dot-multiplication is NOT commutative! In general, $AB \\neq BA$.\n",
    "> \n",
    "> Strictly speaking, this is true for vectors as well. Above, we multiplied the *row*-vector $(a, b)$ by the *column*-vector $(c, d)$. A row-vector is simply a matrix with only one row; a column-vector is simply a matrix with only one column. \n",
    ">\n",
    "> What would be the result of multiplying the column-vector $(c, d)$ on the left by the row-vector $(a, b)$ on the right?\n",
    ">\n",
    "> $$\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        c \\\\\n",
    "        d\n",
    "    \\end{bmatrix}\n",
    "    \\space\n",
    "    \\begin{bmatrix}\n",
    "        a & b\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        ca & cb \\\\\n",
    "        da & db\n",
    "    \\end{bmatrix}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate this difference between $\\begin{bmatrix} a & b \\end{bmatrix} \\begin{bmatrix} c \\\\ d \\end{bmatrix}$ and $\\begin{bmatrix} c \\\\ d \\end{bmatrix} \\begin{bmatrix} a & b \\end{bmatrix}$ in Python for $a=2, b=3, c=4$, and $d=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Solution</summary>\n",
    "    \n",
    "```python\n",
    "    ab = np.array([[2, 3]])\n",
    "    cd = np.array([[4],[5]])\n",
    "    \n",
    "    display(ab @ cd)\n",
    "    display(cd @ ab)\n",
    "    \n",
    "```\n",
    "    \n",
    "</details>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More with Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve an equation like $A\\vec{x} = \\vec{c}$ for $\\vec{x}$, we can't very well divide $\\vec{c}$ by $A$! But there is a notion of matrix _inversion_ that is relevant here, which is analogous to multiplicative inversion. If we have an equation like $2x = 10$, we can simply multiply both sides by the multiplicative inverse of the coefficient of $x$, viz. $2^{-1}$. And here the point, of course, is that $2^{-1} \\times 2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see there is an equivalent _inverse_ for matrices. But first, we should discuss the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **identity matrix** $I$ is the matrix containing 1's along the main diagonal (upper-left to lower-right) and 0's everywhere else:\n",
    ">\n",
    "> $$\\begin{align}\n",
    "    I_3 &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    I_5 &= \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "                           0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "                           0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "                           0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "                           0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "                           0 & 0 & 0 & 0 & 0 & 1 \\\\                           \n",
    "            \\end{bmatrix}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identity matrix is a special matrix since any (compatible) matrix $A$ multiplied by it is the matrix itself: $AI = A$ and $IA = A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Identity Matrices in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I5 = np.eye(5)\n",
    "print(I5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 42*np.ones(25).reshape(5,5)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I5 @ A)\n",
    "print()\n",
    "print(A @ I5)\n",
    "print()\n",
    "is_equal = (I5 @ A) == (A @ I5)\n",
    "print('Both are the same:')\n",
    "print(is_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the higher-dimensional case, what we can do is to left-multiply both sides by the _inverse matrix_ of A, denoted $A^{-1}$, and here the point is that the dot-product $A^{-1}A = I$, where $I$ is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus when we have a matrix equation $A\\vec{x} = \\vec{c}$, we can calculate the solution by multiplying both sides by $A^{-1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "    A\\vec{x}       &= \\vec{c}\\\\\n",
    "    A^{-1}A\\vec{x} &= A^{-1}\\vec{c}\\\\\n",
    "    I \\vec{x}      &= A^{-1}\\vec{c}\\\\\n",
    "    \\vec{x}        &= A^{-1}\\vec{c}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "A = np.array([\n",
    "    [ 1, -2, 3],\n",
    "    [ 2, -5, 10],\n",
    "    [ 0, 0, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can produce the inverse of a matrix in `numpy` by calling `np.linalg.inv()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `numpy` arrays, find the inverse $A^{-1}$ of the matrix below:\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & -2 & 3 \\\\ 2 & -5 & 10 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "> Confirm this is the inverse by multiplying the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    <code>mat = np.array([[1, -2, 3], [2, -5, 10], [0, 0, 1]])\n",
    "mat_inv = np.linalg.inv(mat)\n",
    "mat.dot(mat_inv)</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a System of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a system of equations can take a lot of work\n",
    "\n",
    "$$ \\begin{align}\n",
    " x - 2y + 3z &= 9 \\\\\n",
    " 2x - 5y + 10z &= 4 \\\\\n",
    "  6z &= 0 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the System with Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can make it easier by writing it in matrix form\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix} \n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "    9 \\\\\n",
    "    4 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of this in the abstract:\n",
    "$$ A \\cdot X = B $$\n",
    "$$ A^{-1} \\cdot A \\cdot X = A^{-1} \\cdot B $$\n",
    "$$ I \\cdot X = A^{-1} \\cdot B $$\n",
    "$$ X = A^{-1} \\cdot B $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding It with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try solving the system with matrices:\n",
    "\n",
    "$$ \\begin{align}\n",
    " 1 - 2y + 3z &= 9 \\\\\n",
    " 2x - 5y + 10z &= 4 \\\\\n",
    "  6z &= 0 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the system's matrices:\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix} \n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "    9 \\\\\n",
    "    4 \\\\\n",
    "    0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "to \n",
    "\n",
    "$$ A \\cdot \\vec{X} = B $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([9, 4, 0]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A:')\n",
    "print(A)\n",
    "print()\n",
    "print('B:')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = A_inv @ B\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.dot(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve It Faster with NumPy's `linalg.solve()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's ```linalg``` module has a ```.solve()``` method that you can use to solve a system of linear equations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, it will solve for the vector $\\vec{x}$ in the equation $A\\vec{x} = b$. You should know that, \"under the hood\", the ```.solve()``` method does NOT compute the inverse matrix $A^{-1}$. This is largely because of the enormous expense of directly computing a matrix inverse, which takes $\\mathcal{O}(n^3)$ time.\n",
    "\n",
    "Check out [this discussion](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li) on stackoverflow for more on the differences between using `.solve()` and `.inv()`.\n",
    "\n",
    "And check out the documentation for ```.solve()``` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the .solve() method to solve this system of equations\n",
    "\n",
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])\n",
    "\n",
    "B = np.array([9, 4, 0]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we could just solve our matrix equation by calculating the inverse of our matrix $X$ and then multiplying by $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A).dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the time difference is striking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.inv(A).dot(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.linalg.solve(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a (tiny!) 5x5 matrix, the cost of computing the inverse directly is evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving for the Line of Best Fit: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a typical dataset and the associated multiple linear regression problem. We have many observations (rows), each of which consists of a set of values both for the predictors (columns, i.e. the independent variables) and for the target (the dependent variable).\n",
    "\n",
    "We can think of the values of the independent variables as our matrix $A$ of coefficients and of the values of the dependent variable as our output vector $\\vec{c}$.\n",
    "\n",
    "The task here is, in effect, to solve for $\\vec{\\beta}$, where we have that $A\\vec{\\beta} = \\vec{c}$, except in general we'll have more rows than columns. This is why we won't in general be computing matrix inverses. (They're computationally expensive, anyway.) This is also why we have a problem requiring not a direct solution but rather an optimization--in our case, a best-fit line.\n",
    "\n",
    "Using $z$ for our independent variables and $y$ for our dependent variable, we have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1\\begin{bmatrix}\n",
    "z_{1,1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,1}\n",
    "\\end{bmatrix} +\n",
    "... + \\beta_n\\begin{bmatrix}\n",
    "z_{1,n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,n}\n",
    "\\end{bmatrix} \\approx \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Solves the Best-Fit Line Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\vec{\\beta}$, the vectorized parameters of the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\vec{\\beta} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "$(X^TX)^{-1}X^T$ is sometimes called the *pseudo-inverse* of $X$. We'll have more to say about this in a future lesson when we talk about the singular value decomposition.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(list(zip(np.random.normal(size=10),\n",
    "                          np.array(np.random.normal(size=10, loc=2)))))\n",
    "target = np.array(np.random.exponential(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(preds.T.dot(preds)).dot(preds.T).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(preds, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Cramer's Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cramer's Rule tells us how to solve a system of linear equations in terms of the determinant of the data matrix:\n",
    "\n",
    "If we have $A\\vec{x} = \\vec{b}$, where $\\vec{x} = \\begin{bmatrix}x_1 \\\\ . \\\\ . \\\\ . \\\\ x_n \\end{bmatrix}$, then $x_i = \\frac{|A_i|}{|A|}$,\n",
    "\n",
    "where $A_i$ is the matrix formed by replacing the $i$'th column of $A$ with $\\vec{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the A and B from above.\n",
    "\n",
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])\n",
    "\n",
    "B = np.array([9, 4, 0]).reshape(3, 1)\n",
    "\n",
    "\n",
    "# Cramer's Rule.\n",
    "\n",
    "det_A = np.linalg.det(A)\n",
    "\n",
    "x1_num = np.hstack((B, A[:, 1:]))\n",
    "x2_num = np.hstack((A[:, :1], B, A[:, -1:]))\n",
    "x3_num = np.hstack((A[:, :2], B))\n",
    "\n",
    "print(np.linalg.det(x1_num) / det_A)\n",
    "print(np.linalg.det(x2_num) / det_A)\n",
    "print(np.linalg.det(x3_num) / det_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
